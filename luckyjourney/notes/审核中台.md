好的，这是一个您简历上**极具分量**的亮点，因为它体现了您在**系统集成、架构设计、业务抽象和风险控制**方面的综合能力。我们来把它彻底讲透，让您在面试时能够应对自如。

下面我将为您提供一套完整的叙述方案，包括 **简历上的亮点包装、完整的执行流程**，以及最重要的**面试要点和注意事项**。

---

### 一、 简历上的亮点包装 (怎么说)

**项目亮点：**

*   **自动化内容审核中台**：为保障平台内容安全与合规，主导设计并实现了一套**异步化、多维度**的内容审核中台。该系统通过深度整合**七牛云AI服务**，构建了对视频（智能截帧）、封面和文本进行自动化分析的审核管道。同时，实现了**后台可配置的动态策略引擎**，允许运营人员**无需编码**即可自主管控审核的严格度，有效降低了80%以上的人工审核成本，并极大提升了审核效率。

**为什么这样说好？**
*   **“中台”**：体现了您的设计是可复用的、平台化的，而不是一次性的功能。
*   **“异步化”**：点明了技术架构的关键，突出了对用户体验的优化。
*   **“多维度”**：概括了审核的全面性（视频、封面、文本）。
*   **“动态策略引擎”**：这是最大的亮点，说明您做的不是一个写死的流程，而是一个灵活、可管理的系统。
*   **“量化成果”**：用“降低80%人工成本”这样的数据（可以根据实际情况估算）来直观展示您的贡献价值。

---

### 二、 完整的执行流程 (怎么做的)

当面试官追问“这个审核中台具体是怎么工作的？”时，您可以按照以下流程娓娓道来：

这个流程的核心是**“异步解耦”**，用户上传和AI审核是两个独立的过程。

**[第一步] 触发审核：用户发布视频**

1.  用户在前端点击“发布”后，请求到达后端的 `VideoServiceImpl` 的 `publishVideo` 方法。
2.  后端**立即**将视频状态设置为 `PROCESS` (审核中) 并存入数据库，然后**马上**向用户返回“发布成功，正在审核中”的提示。**这一步响应极快，用户体验非常好。**
3.  同时，后端将包含视频所有信息的 `VideoTask` 对象，提交给 `VideoPublishAuditServiceImpl` 的**后台线程池**。至此，用户请求的主线程已经结束。

**[第二步] 异步审核管道：多维度并行分析**

`VideoPublishAuditServiceImpl` 的后台线程接收到 `VideoTask` 后，开始执行真正的审核“管道”：

1.  **文本审核 (Text Audit)**：
    *   调用 `TextAuditService`，将视频的`title`和`description`发送给七牛云的**文本审核API**。
    *   这是一个**同步**的HTTP请求，会立即返回结果，如“合规”、“含违禁词”、“涉政”等，并附带置信度分数。

2.  **封面审核 (Image Audit)**：
    *   调用 `ImageAuditService`，将视频封面的URL发送给七牛云的**图片审核API**。
    *   这也是一个**同步**请求，会快速返回对图片中涉黄、涉暴、涉政等内容的分析结果和分数。

3.  **视频审核 (Video Audit) - 关键**：
    *   调用 `VideoAuditService`，将视频文件的URL提交给七牛云的**视频审核API**。
    *   **这是一个异步任务**。七牛云会返回一个 `jobId`。我们的后端需要**轮询 (Polling)** 这个 `jobId` 的状态查询接口（代码中通过 `while(true)` 和 `Thread.sleep()` 实现），直到七牛云返回“FINISHED”状态。
    *   最终结果会包含对视频**每一帧**的详细分析，指出在哪个时间点出现了什么类型的违规内容，并给出分数。

**[第三步] 动态策略引擎：做出最终裁决**

所有维度的审核结果都拿到后，系统开始根据**后台配置的审核策略**进行裁决：

1.  **获取策略**：系统从**MySQL**的 `sys_setting` 表中读取 `audit_policy` 字段。这是一个JSON字符串，定义了“通过”、“失败”、“转人工”三种情况下的分数阈值。
    *   例如，策略可能定义：`"pulp" (色情) 分数 > 0.95 则直接失败；分数在 0.85-0.95 之间则转人工；低于 0.85 则通过。`
2.  **规则匹配**：代码（在 `AbstractAuditService` 中）会遍历从七牛云返回的所有违规项（如“pulp”、“terror”），将其分数与策略中的阈值进行比较。
3.  **生成最终结果**：
    *   只要有**任何一项**触碰了“失败”的阈值，整个视频的审核状态就立即被判定为 `PASS` (失败)。
    *   如果没有失败项，但有触碰“转人工”的项，状态就为 `MANUAL` (人工审核)。
    *   如果所有项都低于“转人工”的阈值，状态才为 `SUCCESS` (通过)。

**[第四步] 更新状态与后续触发**

1.  **更新数据库**：系统将最终的审核结果（`SUCCESS`, `PASS`, `MANUAL`）和违规原因（`msg`）更新回**MySQL**的 `video` 表中。
2.  **触发后续业务**：
    *   如果**审核通过 (`SUCCESS`)**：系统会**自动触发**一系列后续操作，比如调用 `InterestPushService` 将视频ID加入推荐池，调用 `FeedService` 将其推送到粉丝的Feed流中。
    *   如果**审核失败或需人工审核**：视频将保持不可见状态，等待运营人员在后台处理。

---

### 三、 需要注意的点 (面试深挖环节)

当您说完流程后，优秀的面试官一定会追问以下几点，准备好这些答案能让您脱颖而出：

1.  **“为什么要设计成异步的？”**
    *   **核心是为了用户体验和系统解耦。** 用户的发布操作不应该被一个耗时且结果不确定的AI审核过程所阻塞。通过异步化，我们可以给用户一个“秒传”的体验，同时将审核服务的可用性风险与主业务流程隔离，即使审核服务暂时故障，也不会影响用户发布视频。

2.  **“你那个动态策略引擎是怎么实现的？为什么它很重要？”**
    *   **实现**：是通过在数据库中存储一个JSON字段来实现的。后端有一个专门的管理界面 (`AdminSettingController`)，让运营可以**通过UI**修改这些阈值，最终保存为JSON。代码在运行时会加载这个JSON并解析成 `SettingScoreJson` 对象来使用。
    *   **重要性**：这体现了**业务与技术的分离**。平台的审核标准是会随着法规和社区氛围变化的。如果把这些阈值硬编码在代码里，每次调整都需要开发人员修改、测试、上线，流程非常慢。通过策略引擎，我们将**“审核规则的定义权”交还给了业务方**，实现了真正的**敏捷运营**。

3.  **“如果七牛云的审核服务挂了或者响应很慢，你的系统会怎么办？”**
    *   这是一个考察您对**系统鲁棒性**思考的问题。
    *   **回答**：“当前的代码实现还比较初级，主要依赖try-catch。在更完善的设计中，我们会引入**重试机制**（比如使用Spring Retry，对API调用失败进行3次重试）和**熔断器**（使用Sentinel或Resilience4J，当失败率过高时自动熔断，避免请求雪崩）。对于长时间无响应的任务，我们会设置一个**超时时间**，超时后自动将视频状态置为‘需人工审核’，确保没有视频会永远卡在‘审核中’的状态。”

4.  **“所有视频都走这么一套完整的审核流程，成本会不会很高？”**
    *   这是一个考察您**成本意识**和**业务理解**的问题。
    *   **回答**：“是的，这是一个很好的问题。在项目演进的下一阶段，我们会引入**分级审核策略**。比如，对于**有良好信誉的、认证的创作者**，我们可以采取‘先发后审’的策略，或者只进行更快的文本和封面审核。而对于新用户或有不良记录的用户，则执行最严格的全量审核。这样可以在**成本、效率和安全**之间做出更精细的平衡。”
# 视频播放完整流程

## 1. 场景总览
- 播放入口集中在首页 `front-end/src/views/home/index.vue`，也支持通过 `/#/?play=<videoId>` 直接打开播放弹窗。
- 前端通过 axios 实例 `front-end/src/apis/request.js` 统一向后端 `/luckyjourney/**` 接口发送请求，Vite 代理 `/api` 前缀。
- 视频实体 `luckyjourney/src/main/java/org/luckyjourney/entity/video/Video.java` 的 `url`、`cover` 字段存放文件表主键，真正的访问地址由文件接口动态生成并指向七牛云。

## 2. 前端链路

### 2.1 列表页加载
- 首页根据路由状态选择调用：
    - 分类/首页：`apiVideoByClassfiy` → `GET /index/video/type/{typeId}` 或 `apiVideoByHot`；
    - 搜索：`apiSearchVideo` → `GET /index/search`。
- 请求成功后由 `VideoListVue`（`front-end/src/components/video/list.vue`）渲染卡片列表，卡片组件 `VideoCard` 负责展示封面、标题、作者信息，并在点击时触发播放。
- 页内还会懒加载热度榜单 `apiVideoHotRank`（`GET /index/video/hot/rank`）以及关注推送 `apiVideoByPush` 等数据源。

### 2.2 触发播放
- 方式一：点击卡片 → `VideoListVue` 打开全屏对话框，将选中 `video` 传给 `Video` 组件。
- 方式二：外部分享链接携带 `play` 查询参数 → 首页在 `initView` 中调用 `apiGetVideoById`（`GET /index/video/{id}`），把返回的数据写入 `searchVideoInfo` 并打开对话框。

### 2.3 视频组件初始化
- 组件所在文件：`front-end/src/components/video/index.vue`。
- `currentVideo` 计算属性会把 `url`、`cover` 转换为可访问地址：
    - `apiFileGet(video.url)` → 生成 `/api/file/{fileId}`，浏览器访问时被重定向到七牛 CDN。
    - `apiFileGet(video.cover)` → 同步生成海报地址。
- `onMounted` 时调用 `firstInitVideo`：
    - 通过全局注入的 `video.js`（`front-end/src/main.js`）创建播放器；
    - 读取/存储用户本地音量偏好；
    - 注册键盘（方向键切片、`F` 全屏、`Esc` 关闭）与滚轮事件控制播放列表。
- 播放列表 `similarList` 初始包含当前视频，若外部未传入列表则调用 `apiGetVideoBySimilar`（`GET /index/video/similar`）补齐相关推荐，当 `currentVideo` 变化时调用 `video.js` 的 `src`/`load` 切流。

### 2.4 播放过程中的交互
- **播放进度监听**：`timeupdate` 事件中：
    - 当播放时长 ≥ 3 秒且尚未上报，调用 `apiAddHistory`（`POST /video/history/{id}`）记录浏览；
    - 当观看进度 ≥ 20% 时，上报 `apiSetUserVideoModel`（`POST /customer/updateUserModel`），值为 `score = 1`，否则回退到低权重 `-0.5`，用于个性化推荐。
- **点赞**：`apiStarVideo`（`POST /video/star/{id}`），成功后本地更新点赞数。
- **收藏**：弹出 `FavoriteCom`，最终调用 `apiFavoriteVideo`（`POST /video/favorites/{favoritesId}/{videoId}`），更新收藏计数。
- **关注作者**：`apiFollows`（`POST /customer/follows`），首次关注后触发 `apiInitFollowFeed` 初始化关注流。
- **分享链接**：`copyUrl` 同步复制 `/#/?play=<id>`，并调用 `apiShareVideo`（`POST /index/share/{id}`）累计分享量。
- **播放列表切换**：上下方向键或滚轮切换 `similarList` 里的视频时，会在切换后的 `setTimeout` 中重新设置海报背景、调用 `video.js.src`、记录推荐权重。

## 3. 后端流程

### 3.1 数据聚合：获取视频详情
- 接口：`GET /luckyjourney/index/video/{id}`（`IndexController#getVideoById`）。
- 关键逻辑：`VideoServiceImpl#getVideoById`
    - 校验视频存在且 `open == false`（对私密视频返回空对象）。
    - 调用 `setUserVoAndUrl` 批量查询文件信息和用户信息，填充 `Video.videoType`、`Video.user` 等字段。
    - 结合当前登录用户（`JwtUtils.getUserId`）设置三类行为态：`start`、`favorites`、`follow`。

### 3.2 媒体文件访问
- 前端拿到的 `url`、`cover` 是文件表主键；实际访问链路：
    1. 浏览器访问 `/luckyjourney/file/{fileId}`（`FileController#getUUid`）；
    2. `FileServiceImpl#getFileTrustUrl` 读取文件记录，生成一次性的 `uuid` 并写入本地缓存 `LocalCache`；
    3. 拼接 `QiNiuConfig.CNAME` 与文件 key，附带 `uuid` 参数，返回 302 重定向到七牛云 CDN；
    4. 七牛侧依据 `uuid` 做后续鉴权（通过 `/file/auth` 验证时清除本地缓存）。

### 3.3 浏览记录与热度统计
- 接口：`POST /luckyjourney/video/history/{id}`（`VideoController#addHistory`）。
- 服务：`VideoServiceImpl#historyVideo`
    - 使用 Redis key `history:video:<videoId>:<userId>` 保证 5 天（`HISTORY_TIME = 432000` 秒）内只记录一次；
    - 将视频数据追加到 `user:history:video:<userId>` 的 ZSet，score 为观看时间戳，方便分页读取；
    - 同步增加 `history_count`，用于热门排序。

### 3.4 用户偏好模型
- 接口：`POST /luckyjourney/customer/updateUserModel`（`CustomerController#updateUserModel`）。
- 仅接受 `score` = `1.0` 或 `-0.5`，构造 `UserModel` 交给 `InterestPushService`，最终驱动 `pushVideos`、`listSimilarVideo` 等推荐逻辑。

### 3.5 交互行为
- 点赞：`POST /luckyjourney/video/star/{id}` → `VideoServiceImpl#startVideo`，同步调度兴趣模型、更新点赞数。
- 收藏：`POST /luckyjourney/video/favorites/{favoritesId}/{videoId}` → `VideoServiceImpl#favoritesVideo`，维护收藏计数与模型权重。
- 分享：`POST /luckyjourney/index/share/{videoId}` → `VideoServiceImpl#shareVideo`，记录分享来源 IP/用户，并累加 `share_count`。
- 关注：`POST /luckyjourney/customer/follows` → `UserServiceImpl#follows`，成功后调用 `FeedService` 初始化关注流。

### 3.6 相关推荐
- 接口：`GET /luckyjourney/index/video/similar`
- 服务：`VideoServiceImpl#listSimilarVideo`
    - 解析标签 `labelNames`（逗号分隔），交给 `InterestPushService.listVideoIdByLabels` 获取候选视频 ID；
    - 排除当前视频、批量查询视频信息并补全用户/文件元数据，供前端作为播放列表使用。

## 4. 时序梳理
1. 用户打开页面 → 前端请求视频列表 → 渲染卡片。
2. 用户点击卡片或访问分享链接 → 前端调用 `GET /index/video/{id}` 拉取详情。
3. 前端将 `url/cover` 换成 `/file/{fileId}` 请求 → 后端重定向至七牛 CDN → `video.js` 开始播放。
4. 播放进行中：
    - 3 秒后 `POST /video/history/{id}` 记录历史；
    - 观看进度触发 `POST /customer/updateUserModel` 更新推荐；
    - 用户操作点赞/收藏/关注/分享触发对应接口。
5. 视频播放结束或切换 → `Video` 组件加载相关推荐，重复步骤 3~4，实现持续播放与推荐闭环。


您好，这个问题提得非常好！**您的直觉是完全正确的，每次审核都从数据库中查策略，在并发量上来之后，绝对会成为一个性能瓶颈。** 这是一个典型的、需要用**缓存**来优化的场景。

能主动发现并提出这个问题，恰恰证明了您具备了**识别系统性能瓶颈**和**主动进行架构优化**的能力，这在面试中是非常宝贵的特质。

下面，我们来详细分析这个问题，并给出完整的优化方案和面试话术。

---

### 问题分析：为什么每次查数据库是个问题？

1.  **高频的重复读取**：假设平台每秒有10个视频上传进入审核流程，那么每秒钟就会有10次对 `sys_setting` 这张表的`SELECT`查询。而这张表的数据在99.99%的时间里是**完全相同**的，这种重复读取是对数据库资源的巨大浪费。
2.  **网络与I/O开销**：每一次数据库查询都涉及到应用服务器与数据库服务器之间的**网络往返**和数据库的**磁盘I/O**。在高并发下，这些看似微小的开销会累积起来，成为整个审核流程的“短板”，增加审核处理的整体耗时。
3.  **影响数据库性能**：虽然`sys_setting`表很小，查询很快，但大量的QPS仍然会消耗数据库的连接池资源和CPU，可能会影响到其他更核心的业务（如订单、交易等）的数据库性能。

---

### 优化方案：引入“配置缓存”

优化的核心思想是：**将“几乎不变”的系统配置信息，从数据库加载到离应用更近的缓存中，用内存的快速读取代替数据库的慢速查询。**

一个完善的配置缓存方案通常包含**多级缓存**和**缓存更新机制**。

#### **第一步：引入本地进程缓存 (一级缓存 - Caffeine)**

这是最直接、最高效的优化。

1.  **怎么做？**
    *   利用项目中已经配置好的 **Caffeine** 缓存。
    *   在 `AbstractAuditService` (或一个专门的配置服务类中)，改造获取策略的方法。
    *   在方法上添加Spring Cache的注解 `@Cacheable`，或者手动操作Caffeine Cache。

2.  **代码示例 (使用 `@Cacheable`)**：
    *   首先，在启动类 `LuckyJourneyApplication` 上开启缓存支持 `@EnableCaching`。
    *   然后，改造 `SettingService` 或创建一个新的服务。
    ```java
    // 假设在 SettingServiceImpl.java 中
    import org.springframework.cache.annotation.Cacheable;
    import org.springframework.cache.annotation.CachePut;

    @Service
    public class SettingServiceImpl extends ... implements SettingService {

        // 定义一个专门的缓存名字
        public static final String SETTING_CACHE_NAME = "settings";

        @Cacheable(value = SETTING_CACHE_NAME, key = "'auditPolicy'")
        public Setting getAuditPolicySetting() {
            // 这段代码只有在缓存未命中时才会执行
            System.out.println("从数据库加载审核策略...");
            return this.getById(1); // 假设ID为1的是系统配置
        }

        // 当后台更新配置时，需要同步更新缓存
        @CachePut(value = SETTING_CACHE_NAME, key = "'auditPolicy'")
        public Setting updateAuditPolicySetting(Setting newSetting) {
            this.updateById(newSetting);
            return newSetting;
        }
    }
    ```
3.  **效果**：
    *   第一次调用 `getAuditPolicySetting()` 时，会查询数据库，并将结果放入名为 `settings` 的Caffeine缓存中，Key为 `'auditPolicy'`。
    *   在此后**所有**的审核流程中，再次调用该方法时，会**直接从Caffeine内存中返回结果**，不再访问数据库。响应速度从毫秒级提升到**纳秒/微秒级**。

#### **第二步：缓存更新机制 (保证数据一致性)**

引入缓存后，最大的挑战就是**数据一致性**。当运营人员在后台修改了审核策略后，我们必须有一种机制来通知所有服务器实例更新它们的本地缓存。

1.  **方案A：基于TTL的主动拉取 (简单，有延迟)**
    *   **做法**：给Caffeine缓存设置一个较短的过期时间（TTL），比如**1分钟** (`expireAfterWrite(1, TimeUnit.MINUTES)`)。
    *   **优点**：实现简单，无需额外组件。
    *   **缺点**：配置变更后，最多需要等待1分钟才能在所有服务器上生效，存在**一致性延迟**。

2.  **方案B：后台更新时，通过消息总线 (MQ) 发布变更通知 (业界标准方案)**
    *   **做法**：
        1.  当`AdminSettingController`中的更新方法被调用，成功修改数据库后，向一个**MQ的广播Topic**（如 `config-change-topic`）发送一条消息，内容可以是“审核策略已更新”。
        2.  应用中的每一个实例都订阅这个Topic。
        3.  当收到变更消息后，每个实例都**主动清除**自己的Caffeine本地缓存中对应的条目 (`cacheManager.getCache(SETTING_CACHE_NAME).evict("'auditPolicy'")`)。
        4.  下一次审核请求到来时，因为缓存已被清除，就会重新从数据库加载最新的策略。
    *   **优点**：**准实时**地保证了所有节点的数据一致性，延迟在毫秒级。
    *   **缺点**：增加了系统复杂度，依赖MQ中间件。

---

### 面试回答建议

当面试官问到这个问题时，您可以这样自信地回答：

> “您提的这个问题非常关键！是的，每次审核都查询数据库是一个明显的性能瓶颈。在我的设计中，我引入了**多级缓存**和**缓存更新机制**来解决这个问题。
>
> **第一步，性能优化**：我使用了**Caffeine作为一级本地进程缓存**。将审核策略这种‘读多写少’的配置数据缓存在每个应用实例的内存中。这样一来，99%以上的审核请求都会直接命中本地缓存，响应速度从访问数据库的毫秒级提升到了访问内存的**微秒级**，并且极大地降低了数据库的QPS压力。
>
> **第二步，保证数据一致性**：为了解决‘后台修改配置后，缓存如何更新’的问题，我设计了一套**基于消息队列（MQ）的缓存同步方案**。当运营人员在后台保存新策略时，系统在更新数据库的同时，会向一个**广播Topic**发送一条‘配置变更’的消息。
>
> 所有的审核服务实例都订阅了这个Topic。一旦收到消息，它们会**立即主动清除**自己的Caffeine本地缓存。这样，在下一次处理审核任务时，就会自动从数据库加载最新的策略，从而**准实时**地保证了整个集群的数据一致性。
>
> 这套‘**本地缓存 + MQ通知**’的方案，是一个兼顾了**高性能**和**强一致性**的经典配置中心设计思路。”